1. Scrapper folder project:
`scrapy startproject <project_name>`

2. Python spider file for scraping:
`scrapy genspider <python_filename> <utl_link>`

3. Enable scrapy shell feature by inserting ipython into the scrapy.cfg.
The purpose of this step is to test you selector in interactive way. How to do so?
set connection to the url by running the `fetch('<url>')` command.

4. Perform scrapping we can also go to other links within the website if there's another page.
you can follow the link by using the response.follow() command

5. Using Item object to set what data we want to return. 
Pipelines used for cleaning data:
a. You need to use adapter as interface to access and modify values without modifying the source code
b. go to settings.py and enable the pipeline

6. We can set where we want to save our data by go into the settings.py and add
FEEDS = {
    'data.json': {'format': 'json'}
}

8. Some websites are able to detect if the same person/ agent perform multiple requests and block it.
To go around that problem we need help from generated user agents, posing as different people performing
requests so that we can scrape without any blocker.

To do so you need to go to the settings.py and add multiple user agents (get from the scrapeops api) 
and then add it to each header document inside the response.follow() function. Some websites requires
use to change multiple header elements because they check the whole header not just the user agents.

For configuring user agents we can use the middlewares.py
1. Fetch user agent/ browser headers from the scrapeops api
2. the main function that you need to add is the process_request() function
